{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAEvaluator():\n",
    "    def __init__(self, model_dir=None):\n",
    "\n",
    "        QAE_PRETRAINED = 'iarfmoose/bert-base-cased-qa-evaluator'\n",
    "        self.SEQ_LENGTH = 512\n",
    "\n",
    "        self.device = torch.device('cpu')\n",
    "        # self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.qae_tokenizer = AutoTokenizer.from_pretrained(QAE_PRETRAINED)\n",
    "        self.qae_model = AutoModelForSequenceClassification.from_pretrained(QAE_PRETRAINED)\n",
    "        self.qae_model.to(self.device)\n",
    "\n",
    "\n",
    "    def encode_qa_pairs(self, questions, answers):\n",
    "        encoded_pairs = []\n",
    "        for i in range(len(questions)):\n",
    "            encoded_qa = self._encode_qa(questions[i], answers[i])\n",
    "            encoded_pairs.append(encoded_qa.to(self.device))\n",
    "        return encoded_pairs\n",
    "\n",
    "    def get_scores(self, encoded_qa_pairs):\n",
    "        scores = {}\n",
    "        self.qae_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(encoded_qa_pairs)):\n",
    "                scores[i] = self._evaluate_qa(encoded_qa_pairs[i])\n",
    "\n",
    "        return [k for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)]\n",
    "\n",
    "    def _encode_qa(self, question, answer):\n",
    "        if type(answer) is list:\n",
    "            for a in answer:\n",
    "                if a['correct']:\n",
    "                    correct_answer = a['answer']\n",
    "        else:\n",
    "            correct_answer = answer\n",
    "        return self.qae_tokenizer(\n",
    "            text=question,\n",
    "            text_pair=correct_answer,\n",
    "            pad_to_max_length=True,\n",
    "            max_length=self.SEQ_LENGTH,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def _evaluate_qa(self, encoded_qa_pair):\n",
    "        output = self.qae_model(**encoded_qa_pair)\n",
    "        return output[0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_evaluator = QAEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammar-based QG Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What is Windows?\n",
      "Answer:   \n",
      "Examples of Operating Systems are Windows, Linux, Mac OS, etc\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "generated_questions = []\n",
    "qg_answers = []\n",
    "\n",
    "with open('result-0-grammar-based-qg.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "for item in data:\n",
    "    question = item['Question']\n",
    "    answer = item['Answer']\n",
    "    \n",
    "    generated_questions.append(question)\n",
    "    qg_answers.append(answer)\n",
    "\n",
    "print(\"Question: \", generated_questions[0])\n",
    "print(\"Answer: \", qg_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaushalpatil/Development/Question Generation Paper/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "grammar_encoded_qa_pairs = qa_evaluator.encode_qa_pairs(generated_questions, qg_answers)\n",
    "grammar_scores = qa_evaluator.get_scores(grammar_encoded_qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 9, 7, 5, 3, 8, 0, 6, 2, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "print(grammar_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PKE Question Generation Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What is one of the most important parts of a computer?\n",
      "Answer:  Operating system\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "generated_questions = []\n",
    "qg_answers = []\n",
    "\n",
    "with open('result-1-pke-qg-model.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "for item in data:\n",
    "    question = item['Question']\n",
    "    answer = item['answer']\n",
    "    \n",
    "    generated_questions.append(question)\n",
    "    qg_answers.append(answer)\n",
    "\n",
    "print(\"Question: \", generated_questions[0])\n",
    "print(\"Answer: \", qg_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaushalpatil/Development/Question Generation Paper/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pke_encoded_qa_pairs = qa_evaluator.encode_qa_pairs(generated_questions, qg_answers)\n",
    "pke_scores = qa_evaluator.get_scores(pke_encoded_qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4, 2, 5, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "print(pke_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5 Base Question Generation Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What is the most common operating system?\n",
      "Answer:  Windows\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "generated_questions = []\n",
    "qg_answers = []\n",
    "\n",
    "with open('result-2-t5-base-qg.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "for item in data:\n",
    "    question = item['Question']\n",
    "    answer = item['answer']\n",
    "    \n",
    "    generated_questions.append(question)\n",
    "    qg_answers.append(answer)\n",
    "\n",
    "print(\"Question: \", generated_questions[0])\n",
    "print(\"Answer: \", qg_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaushalpatil/Development/Question Generation Paper/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t5_encoded_qa_pairs = qa_evaluator.encode_qa_pairs(generated_questions, qg_answers)\n",
    "t5_scores = qa_evaluator.get_scores(t5_encoded_qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 5, 4, 3, 7, 6, 9, 8]\n"
     ]
    }
   ],
   "source": [
    "print(t5_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KeyBERT Question Generation Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What is the abbreviation for an operating system?\n",
      "Answer:  Operating systems\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "generated_questions = []\n",
    "qg_answers = []\n",
    "\n",
    "with open('result-3-keybert-qg-model.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "for item in data:\n",
    "    question = item['Question']\n",
    "    answer = item['answer']\n",
    "    \n",
    "    generated_questions.append(question)\n",
    "    qg_answers.append(answer)\n",
    "\n",
    "print(\"Question: \", generated_questions[0])\n",
    "print(\"Answer: \", qg_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaushalpatil/Development/Question Generation Paper/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "keybert_encoded_qa_pairs = qa_evaluator.encode_qa_pairs(generated_questions, qg_answers)\n",
    "keybert_scores = qa_evaluator.get_scores(keybert_encoded_qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print(keybert_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini-based Question Generation Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What is the Operating System?\n",
      "Answer:  System software\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "generated_questions = []\n",
    "qg_answers = []\n",
    "\n",
    "with open('result-5-gemini-prompt-qg-1.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "for item in data:\n",
    "    question = item['question']\n",
    "    answer = item['answer']\n",
    "    \n",
    "    generated_questions.append(question)\n",
    "    qg_answers.append(answer)\n",
    "\n",
    "print(\"Question: \", generated_questions[0])\n",
    "print(\"Answer: \", qg_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaushalpatil/Development/Question Generation Paper/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gemini_encoded_qa_pairs = qa_evaluator.encode_qa_pairs(generated_questions, qg_answers)\n",
    "gemini_scores = qa_evaluator.get_scores(gemini_encoded_qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 2, 3, 5, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(gemini_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral-based Question Generation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "generated_questions = []\n",
    "qg_answers = []\n",
    "\n",
    "with open('result-6-mistral-7B-instruct-v2.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question_data in data['questions']:\n",
    "    generated_questions.append(question_data['question'])\n",
    "    qg_answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaushalpatil/Development/Question Generation Paper/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mistral_encoded_qa_pairs = qa_evaluator.encode_qa_pairs(generated_questions, qg_answers)\n",
    "mistral_scores = qa_evaluator.get_scores(mistral_encoded_qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 0, 8, 7, 1, 9, 4, 2, 11, 10, 12, 3, 6]\n"
     ]
    }
   ],
   "source": [
    "print(mistral_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
