{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: flashtext in ./venv/lib/python3.10/site-packages (2.7)\n",
                        "Requirement already satisfied: torch in ./venv/lib/python3.10/site-packages (2.2.1)\n",
                        "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from torch) (3.13.1)\n",
                        "Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.10/site-packages (from torch) (4.9.0)\n",
                        "Requirement already satisfied: sympy in ./venv/lib/python3.10/site-packages (from torch) (1.12)\n",
                        "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch) (3.2.1)\n",
                        "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch) (3.1.3)\n",
                        "Requirement already satisfied: fsspec in ./venv/lib/python3.10/site-packages (from torch) (2024.2.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
                        "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
                        "Requirement already satisfied: sentencepiece in ./venv/lib/python3.10/site-packages (0.2.0)\n",
                        "Requirement already satisfied: transformers in ./venv/lib/python3.10/site-packages (4.38.1)\n",
                        "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
                        "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./venv/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
                        "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
                        "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from transformers) (23.2)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
                        "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
                        "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./venv/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
                        "Requirement already satisfied: safetensors>=0.4.1 in ./venv/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
                        "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
                        "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
                        "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests->transformers) (2.2.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
                        "Requirement already satisfied: sentence-transformers in ./venv/lib/python3.10/site-packages (2.4.0)\n",
                        "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in ./venv/lib/python3.10/site-packages (from sentence-transformers) (4.38.1)\n",
                        "Requirement already satisfied: tqdm in ./venv/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\n",
                        "Requirement already satisfied: torch>=1.11.0 in ./venv/lib/python3.10/site-packages (from sentence-transformers) (2.2.1)\n",
                        "Requirement already satisfied: numpy in ./venv/lib/python3.10/site-packages (from sentence-transformers) (1.26.4)\n",
                        "Requirement already satisfied: scikit-learn in ./venv/lib/python3.10/site-packages (from sentence-transformers) (1.4.0)\n",
                        "Requirement already satisfied: scipy in ./venv/lib/python3.10/site-packages (from sentence-transformers) (1.12.0)\n",
                        "Requirement already satisfied: huggingface-hub>=0.15.1 in ./venv/lib/python3.10/site-packages (from sentence-transformers) (0.20.3)\n",
                        "Requirement already satisfied: Pillow in ./venv/lib/python3.10/site-packages (from sentence-transformers) (10.2.0)\n",
                        "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
                        "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.2.0)\n",
                        "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\n",
                        "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
                        "Requirement already satisfied: sympy in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
                        "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
                        "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
                        "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
                        "Requirement already satisfied: safetensors>=0.4.1 in ./venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
                        "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
                        "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
                        "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
                        "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
                        "Requirement already satisfied: sense2vec in ./venv/lib/python3.10/site-packages (2.0.2)\n",
                        "Requirement already satisfied: spacy<4.0.0,>=3.0.0 in ./venv/lib/python3.10/site-packages (from sense2vec) (3.7.4)\n",
                        "Requirement already satisfied: wasabi<1.2.0,>=0.8.1 in ./venv/lib/python3.10/site-packages (from sense2vec) (1.1.2)\n",
                        "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in ./venv/lib/python3.10/site-packages (from sense2vec) (2.4.8)\n",
                        "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in ./venv/lib/python3.10/site-packages (from sense2vec) (2.0.10)\n",
                        "Requirement already satisfied: numpy>=1.15.0 in ./venv/lib/python3.10/site-packages (from sense2vec) (1.26.4)\n",
                        "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./venv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (3.0.12)\n",
                        "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./venv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (1.0.5)\n",
                        "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (1.0.10)\n",
                        "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (2.0.8)\n",
                        "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (3.0.9)\n",
                        "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in ./venv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (8.2.3)\n",
                        "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in ./venv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (0.3.4)\n",
                        "Requirement already satisfied: typer<0.10.0,>=0.3.0 in ./venv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (0.9.0)\n",
                        "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./venv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (6.4.0)\n",
                        "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (4.66.1)\n",
                        "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (2.31.0)\n",
                        "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./venv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (2.6.1)\n",
                        "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (3.1.3)\n",
                        "Requirement already satisfied: setuptools in ./venv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (65.5.0)\n",
                        "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (23.2)\n",
                        "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./venv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (3.3.0)\n",
                        "Requirement already satisfied: annotated-types>=0.4.0 in ./venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->sense2vec) (0.6.0)\n",
                        "Requirement already satisfied: pydantic-core==2.16.2 in ./venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->sense2vec) (2.16.2)\n",
                        "Requirement already satisfied: typing-extensions>=4.6.1 in ./venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->sense2vec) (4.9.0)\n",
                        "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (3.3.2)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (3.6)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (2.2.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (2024.2.2)\n",
                        "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./venv/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->sense2vec) (0.7.11)\n",
                        "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./venv/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->sense2vec) (0.1.4)\n",
                        "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./venv/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<4.0.0,>=3.0.0->sense2vec) (8.1.7)\n",
                        "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in ./venv/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<4.0.0,>=3.0.0->sense2vec) (0.16.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->spacy<4.0.0,>=3.0.0->sense2vec) (2.1.5)\n"
                    ]
                }
            ],
            "source": [
                "! pip install flashtext\n",
                "! pip install torch\n",
                "! pip install sentencepiece\n",
                "! pip install transformers\n",
                "! pip install sentence-transformers\n",
                "! pip install sense2vec"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting transformers==4.8.1\n",
                        "  Downloading transformers-4.8.1-py3-none-any.whl.metadata (48 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting sentencepiece==0.1.95\n",
                        "  Using cached sentencepiece-0.1.95.tar.gz (508 kB)\n",
                        "  Installing build dependencies ... \u001b[?25ldone\n",
                        "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
                        "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
                        "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
                        "\u001b[?25hRequirement already satisfied: flashtext==2.7 in ./venv/lib/python3.10/site-packages (2.7)\n",
                        "Collecting sentence-transformers==2.2.2\n",
                        "  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n",
                        "  Installing build dependencies ... \u001b[?25ldone\n",
                        "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
                        "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
                        "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
                        "\u001b[?25hCollecting sense2vec==2.0.0\n",
                        "  Using cached sense2vec-2.0.0-py2.py3-none-any.whl (39 kB)\n",
                        "Collecting textwrap3==0.9.2\n",
                        "  Using cached textwrap3-0.9.2-py2.py3-none-any.whl (12 kB)\n",
                        "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from transformers==4.8.1) (3.13.1)\n",
                        "Collecting huggingface-hub==0.0.12 (from transformers==4.8.1)\n",
                        "  Downloading huggingface_hub-0.0.12-py3-none-any.whl.metadata (5.6 kB)\n",
                        "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.10/site-packages (from transformers==4.8.1) (1.26.4)\n",
                        "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (from transformers==4.8.1) (23.2)\n",
                        "Requirement already satisfied: pyyaml in ./venv/lib/python3.10/site-packages (from transformers==4.8.1) (6.0.1)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.10/site-packages (from transformers==4.8.1) (2023.12.25)\n",
                        "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from transformers==4.8.1) (2.31.0)\n",
                        "Collecting sacremoses (from transformers==4.8.1)\n",
                        "  Using cached sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
                        "Collecting tokenizers<0.11,>=0.10.1 (from transformers==4.8.1)\n",
                        "  Using cached tokenizers-0.10.3.tar.gz (212 kB)\n",
                        "  Installing build dependencies ... \u001b[?25ldone\n",
                        "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
                        "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
                        "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.10/site-packages (from transformers==4.8.1) (4.66.1)\n",
                        "Requirement already satisfied: torch>=1.6.0 in ./venv/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (2.2.1)\n",
                        "Collecting torchvision (from sentence-transformers==2.2.2)\n",
                        "  Using cached torchvision-0.17.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
                        "Requirement already satisfied: scikit-learn in ./venv/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.4.0)\n",
                        "Requirement already satisfied: scipy in ./venv/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.12.0)\n",
                        "Requirement already satisfied: nltk in ./venv/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
                        "INFO: pip is looking at multiple versions of sentence-transformers to determine which version is compatible with other requirements. This could take a while.\n",
                        "\u001b[31mERROR: Cannot install sentence-transformers==2.2.2 and transformers==4.8.1 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
                        "\u001b[0m\n",
                        "The conflict is caused by:\n",
                        "    transformers 4.8.1 depends on huggingface-hub==0.0.12\n",
                        "    sentence-transformers 2.2.2 depends on huggingface-hub>=0.4.0\n",
                        "\n",
                        "To fix this you could try to:\n",
                        "1. loosen the range of package versions you've specified\n",
                        "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
                        "\n",
                        "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
                        "\u001b[0mCollecting git+https://github.com/boudinfl/pke.git\n",
                        "  Cloning https://github.com/boudinfl/pke.git to /private/var/folders/xx/cq2k_b4j60l3mtp2k3x41s0w0000gn/T/pip-req-build-1hn6cf9a\n",
                        "  Running command git clone --filter=blob:none --quiet https://github.com/boudinfl/pke.git /private/var/folders/xx/cq2k_b4j60l3mtp2k3x41s0w0000gn/T/pip-req-build-1hn6cf9a\n",
                        "  Resolved https://github.com/boudinfl/pke.git to commit 69871ffdb720b83df23684fea53ec8776fd87e63\n",
                        "  Installing build dependencies ... \u001b[?25ldone\n",
                        "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
                        "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
                        "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
                        "\u001b[?25hRequirement already satisfied: nltk in ./venv/lib/python3.10/site-packages (from pke==2.0.0) (3.8.1)\n",
                        "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from pke==2.0.0) (3.2.1)\n",
                        "Requirement already satisfied: numpy in ./venv/lib/python3.10/site-packages (from pke==2.0.0) (1.26.4)\n",
                        "Requirement already satisfied: scipy in ./venv/lib/python3.10/site-packages (from pke==2.0.0) (1.12.0)\n",
                        "Requirement already satisfied: scikit-learn in ./venv/lib/python3.10/site-packages (from pke==2.0.0) (1.4.0)\n",
                        "Requirement already satisfied: unidecode in ./venv/lib/python3.10/site-packages (from pke==2.0.0) (1.3.8)\n",
                        "Requirement already satisfied: future in ./venv/lib/python3.10/site-packages (from pke==2.0.0) (1.0.0)\n",
                        "Requirement already satisfied: joblib in ./venv/lib/python3.10/site-packages (from pke==2.0.0) (1.3.2)\n",
                        "Requirement already satisfied: spacy>=3.2.3 in ./venv/lib/python3.10/site-packages (from pke==2.0.0) (3.7.4)\n",
                        "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./venv/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.12)\n",
                        "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./venv/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.5)\n",
                        "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.10)\n",
                        "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.8)\n",
                        "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.9)\n",
                        "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in ./venv/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (8.2.3)\n",
                        "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./venv/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (1.1.2)\n",
                        "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./venv/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (2.4.8)\n",
                        "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./venv/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.10)\n",
                        "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in ./venv/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (0.3.4)\n",
                        "Requirement already satisfied: typer<0.10.0,>=0.3.0 in ./venv/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (0.9.0)\n",
                        "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./venv/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (6.4.0)\n",
                        "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (4.66.1)\n",
                        "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (2.31.0)\n",
                        "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./venv/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (2.6.1)\n",
                        "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (3.1.3)\n",
                        "Requirement already satisfied: setuptools in ./venv/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (65.5.0)\n",
                        "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (23.2)\n",
                        "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./venv/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (3.3.0)\n",
                        "Requirement already satisfied: click in ./venv/lib/python3.10/site-packages (from nltk->pke==2.0.0) (8.1.7)\n",
                        "Requirement already satisfied: regex>=2021.8.3 in ./venv/lib/python3.10/site-packages (from nltk->pke==2.0.0) (2023.12.25)\n",
                        "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.10/site-packages (from scikit-learn->pke==2.0.0) (3.2.0)\n",
                        "Requirement already satisfied: annotated-types>=0.4.0 in ./venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (0.6.0)\n",
                        "Requirement already satisfied: pydantic-core==2.16.2 in ./venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (2.16.2)\n",
                        "Requirement already satisfied: typing-extensions>=4.6.1 in ./venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (4.9.0)\n",
                        "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.3.2)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.6)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.2.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2024.2.2)\n",
                        "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./venv/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2.3->pke==2.0.0) (0.7.11)\n",
                        "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./venv/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2.3->pke==2.0.0) (0.1.4)\n",
                        "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in ./venv/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (0.16.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->spacy>=3.2.3->pke==2.0.0) (2.1.5)\n",
                        "Processing /home/conda/feedstock_root/build_artifacts/appnope_1649077682618/work (from -r requirements.txt (line 4))\n",
                        "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/System/Volumes/Data/home/conda/feedstock_root/build_artifacts/appnope_1649077682618/work'\n",
                        "\u001b[0m\u001b[31m\n",
                        "\u001b[0m"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/kaushalpatil/Development/Question Generation Paper/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n",
                        "/Users/kaushalpatil/Development/Question Generation Paper/venv/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
                        "  _torch_pytree._register_pytree_node(\n",
                        "/Users/kaushalpatil/Development/Question Generation Paper/venv/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
                        "  _torch_pytree._register_pytree_node(\n",
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     /Users/kaushalpatil/nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package brown to\n",
                        "[nltk_data]     /Users/kaushalpatil/nltk_data...\n",
                        "[nltk_data]   Package brown is already up-to-date!\n",
                        "[nltk_data] Downloading package wordnet to\n",
                        "[nltk_data]     /Users/kaushalpatil/nltk_data...\n",
                        "[nltk_data]   Package wordnet is already up-to-date!\n",
                        "[nltk_data] Downloading package stopwords to\n",
                        "[nltk_data]     /Users/kaushalpatil/nltk_data...\n",
                        "[nltk_data]   Package stopwords is already up-to-date!\n"
                    ]
                }
            ],
            "source": [
                "! pip install transformers==4.8.1 sentencepiece==0.1.95 flashtext==2.7 sentence-transformers==2.2.2 sense2vec==2.0.0 textwrap3==0.9.2 flashtext\n",
                "! pip install git+https://github.com/boudinfl/pke.git\n",
                "! pip install -r requirements.txt\n",
                "from flashtext import KeywordProcessor\n",
                "import torch\n",
                "from transformers import T5ForConditionalGeneration, T5Tokenizer, pipeline, BartTokenizer, BartForConditionalGeneration\n",
                "import nltk\n",
                "from nltk.corpus import wordnet as wn\n",
                "from nltk.tokenize import sent_tokenize\n",
                "from nltk.corpus import stopwords\n",
                "import string\n",
                "import pke\n",
                "import traceback\n",
                "import spacy\n",
                "from sense2vec import Sense2Vec\n",
                "import numpy as np\n",
                "import sentencepiece\n",
                "spacy_model = spacy.load('en_core_web_sm')\n",
                "\n",
                "nltk.download('punkt')\n",
                "nltk.download('brown')\n",
                "nltk.download('wordnet')\n",
                "nltk.download('stopwords')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# from similarity.normalized_levenshtein import NormalizedLevenshtein\n",
                "# normalized_levenshtein = NormalizedLevenshtein()\n",
                "from sentence_transformers import SentenceTransformer\n",
                "sentence_transformer_model = SentenceTransformer('msmarco-distilbert-base-v3')\n",
                "from collections import OrderedDict\n",
                "from sklearn.metrics.pairwise import cosine_similarity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
                    ]
                }
            ],
            "source": [
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "summary_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
                "summary_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
                "\n",
                "question_model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
                "question_tokenizer = T5Tokenizer.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
                "question_model = question_model.to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def summarizer(text, high = 500, low = 250):\n",
                "    text = text.strip().replace(\"\\n\",\" \")\n",
                "    text = \"summarize: \"+text\n",
                "    input_tokens = summary_tokenizer.batch_encode_plus ([text], return_tensors='pt', max_length=1024, truncation=True)['input_ids']\n",
                "    encoded_ids = summary_model.generate (input_tokens, num_beams=4, length_penalty=2.0, max_length=high, min_length=low, no_repeat_ngram_size=3)\n",
                "    summary = summary_tokenizer.decode(encoded_ids. squeeze (), skip_special_tokens=True)\n",
                "    print(summary)\n",
                "    return summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_nouns_multipartite(content):\n",
                "    out=[]\n",
                "    try:\n",
                "        extractor = pke.unsupervised.MultipartiteRank()\n",
                "        extractor.load_document(input=content,language='en', spacy_model = spacy_model)\n",
                "        pos = {'PROPN','NOUN'}\n",
                "        stoplist = list(string.punctuation)\n",
                "        stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
                "        stoplist += stopwords.words('english')\n",
                "        extractor.candidate_selection(pos=pos)\n",
                "        extractor.candidate_weighting(alpha=1.1,threshold=0.75,method='average')\n",
                "        keyphrases = extractor.get_n_best(n=15)\n",
                "        for val in keyphrases:\n",
                "            out.append(val[0])\n",
                "    except:\n",
                "        out = []\n",
                "        traceback.print_exc()\n",
                "    print(out)\n",
                "    return out"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_keywords(originaltext, summarytext):\n",
                "    keywords = get_nouns_multipartite(originaltext)\n",
                "    print (\"keywords unsummarized: \",keywords)\n",
                "    keyword_processor = KeywordProcessor()\n",
                "    for keyword in keywords:\n",
                "        keyword_processor.add_keyword(keyword)\n",
                "\n",
                "    keywords_found = keyword_processor.extract_keywords(summarytext)\n",
                "    keywords_found = list(set(keywords_found))\n",
                "    print (\"keywords_found in summarized: \",keywords_found)\n",
                "\n",
                "    important_keywords =[]\n",
                "    for keyword in keywords:\n",
                "        if keyword in keywords_found:\n",
                "            important_keywords.append(keyword)\n",
                "    return important_keywords\n",
                "\n",
                "# imp_keywords = get_keywords(text, summarized_text)\n",
                "# print(imp_keywords)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_question(context, answer, model, tokenizer):\n",
                "    text = \"context: {} answer: {}\".format(context, answer)\n",
                "    encoding = tokenizer.encode_plus(\n",
                "        text, max_length=384, pad_to_max_length=False, truncation=True, return_tensors=\"pt\"\n",
                "    ).to(device)\n",
                "    input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
                "    outs = model.generate(\n",
                "        input_ids=input_ids,\n",
                "        attention_mask=attention_mask,\n",
                "        early_stopping=True,\n",
                "        num_beams=5,\n",
                "        num_return_sequences=1,\n",
                "        no_repeat_ngram_size=2,\n",
                "        max_length=72,\n",
                "    )\n",
                "    dec = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
                "    Question = dec[0].replace(\"question:\", \"\")\n",
                "    Question = Question.strip()\n",
                "    return Question"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = '''Operating System lies in the category of system software. \n",
                "It basically manages all the resources of the computer. \n",
                "An operating system acts as an interface between the software and different parts of the computer or the computer hardware. \n",
                "The operating system is designed in such a way that it can manage the overall resources and operations of the computer. \n",
                "Operating System is a fully integrated set of specialized programs that handle all the operations of the computer. \n",
                "It controls and monitors the execution of all other programs that reside in the computer, which also includes application programs and other system software of the computer. \n",
                "Examples of Operating Systems are Windows, Linux, Mac OS, etc. \n",
                "An Operating System (OS) is a collection of software that manages computer hardware resources and provides common services for computer programs. \n",
                "The operating system is the most important type of system software in a computer system. \n",
                "The operating system helps in improving the computer software as well as hardware. \n",
                "Without OS, it became very difficult for any application to be user-friendly. \n",
                "The Operating System provides a user with an interface that makes any application attractive and user-friendly. \n",
                "The operating System comes with a large number of device drivers that make OS services reachable to the hardware environment. \n",
                "Each and every application present in the system requires the Operating System. \n",
                "The operating system works as a communication channel between system hardware and system software. \n",
                "The operating system helps an application with the hardware part without knowing about the actual hardware configuration. \n",
                "It is one of the most important parts of the system and hence it is present in every device, whether large or small device. \n",
                "Functions of the Operating System: Resource Management: The operating system manages and allocates memory, CPU time, and other hardware resources among the various programs and processes running on the computer. \n",
                "Process Management: The operating system is responsible for starting, stopping, and managing processes and programs. It also controls the scheduling of processes and allocates resources to them. \n",
                "Memory Management: The operating system manages the computer's primary memory and provides mechanisms for optimizing memory usage. \n",
                "Security: The operating system provides a secure environment for the user, applications, and data by implementing security policies and mechanisms such as access controls and encryption. \n",
                "Job Accounting: It keeps track of time and resources used by various jobs or users. \n",
                "File Management: The operating system is responsible for organizing and managing the file system, including the creation, deletion, and manipulation of files and directories. \n",
                "Device Management: The operating system manages input/output devices such as printers, keyboards, mice, and displays. It provides the necessary drivers and interfaces to enable communication between the devices and the computer. \n",
                "Networking: The operating system provides networking capabilities such as establishing and managing network connections, handling network protocols, and sharing resources such as printers and files over a network. \n",
                "User Interface: The operating system provides a user interface that enables users to interact with the computer system. This can be a Graphical User Interface (GUI), a Command-Line Interface (CLI), or a combination of both. \n",
                "Backup and Recovery: The operating system provides mechanisms for backing up data and recovering it in case of system failures, errors, or disasters. \n",
                "Virtualization: The operating system provides virtualization capabilities that allow multiple operating systems or applications to run on a single physical machine. This can enable efficient use of resources and flexibility in managing workloads. \n",
                "Performance Monitoring: The operating system provides tools for monitoring and optimizing system performance, including identifying bottlenecks, optimizing resource usage, and analyzing system logs and metrics. \n",
                "Time-Sharing: The operating system enables multiple users to share a computer system and its resources simultaneously by providing time-sharing mechanisms that allocate resources fairly and efficiently. \n",
                "System Calls: The operating system provides a set of system calls that enable applications to interact with the operating system and access its resources. System calls provide a standardized interface between applications and the operating system, enabling portability and compatibility across different hardware and software platforms. \n",
                "Error-detecting Aids: These contain methods that include the production of dumps, traces, error messages, and other debugging and error-detecting methods.'''"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'summarizer' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[2], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m         questions\u001b[38;5;241m.\u001b[39mappend(question_data)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m questions\n\u001b[0;32m---> 17\u001b[0m ans \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_mcqs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
                        "Cell \u001b[0;32mIn[2], line 2\u001b[0m, in \u001b[0;36mgenerate_mcqs\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_mcqs\u001b[39m(text):\n\u001b[0;32m----> 2\u001b[0m     summarized_text \u001b[38;5;241m=\u001b[39m \u001b[43msummarizer\u001b[49m(text)\n\u001b[1;32m      3\u001b[0m     imp_keywords \u001b[38;5;241m=\u001b[39m get_keywords(text, summarized_text)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'summarizer' is not defined"
                    ]
                }
            ],
            "source": [
                "def generate_mcqs(text):\n",
                "    summarized_text = summarizer(text)\n",
                "    imp_keywords = get_keywords(text, summarized_text)\n",
                "    id = 0\n",
                "    questions = []\n",
                "    for answer in imp_keywords:\n",
                "        ques = get_question(summarized_text, answer, question_model, question_tokenizer)\n",
                "        id = len(questions) + 1\n",
                "        question_data = {\n",
                "            \"ID\": id,  \n",
                "            \"Question\": ques,\n",
                "            \"answer\": answer.capitalize(),\n",
                "        }\n",
                "        questions.append(question_data)\n",
                "    return questions\n",
                "\n",
                "ans = generate_mcqs(text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "with open('result-0.json', 'w') as fp:\n",
                "    json.dump(ans, fp)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[\n",
                        "    {\n",
                        "        \"ID\": 1,\n",
                        "        \"Question\": \"What is one of the most important parts of a computer?\",\n",
                        "        \"answer\": \"Operating system\"\n",
                        "    },\n",
                        "    {\n",
                        "        \"ID\": 2,\n",
                        "        \"Question\": \"The operating system is one of the most important parts of what?\",\n",
                        "        \"answer\": \"System\"\n",
                        "    },\n",
                        "    {\n",
                        "        \"ID\": 3,\n",
                        "        \"Question\": \"An operating system is designed to manage the overall resources and operations of what?\",\n",
                        "        \"answer\": \"Computer\"\n",
                        "    },\n",
                        "    {\n",
                        "        \"ID\": 4,\n",
                        "        \"Question\": \"An operating system can enable efficient use of what?\",\n",
                        "        \"answer\": \"Resources\"\n",
                        "    },\n",
                        "    {\n",
                        "        \"ID\": 5,\n",
                        "        \"Question\": \"What does the operating system communicate with?\",\n",
                        "        \"answer\": \"System software\"\n",
                        "    },\n",
                        "    {\n",
                        "        \"ID\": 6,\n",
                        "        \"Question\": \"An operating system provides a secure environment for who?\",\n",
                        "        \"answer\": \"User\"\n",
                        "    }\n",
                        "]\n"
                    ]
                }
            ],
            "source": [
                "with open('result-0.json', 'r') as f:\n",
                "    data = json.load(f)\n",
                "print(json.dumps(data, indent= 4))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Distractor Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def filter_same_sense_words(original,wordlist):\n",
                "    filtered_words=[]\n",
                "    base_sense =original.split('|')[1]\n",
                "    # print (base_sense)\n",
                "    for eachword in wordlist:\n",
                "        if eachword[0].split('|')[1] == base_sense:\n",
                "            filtered_words.append(eachword[0].split('|')[0].replace(\"_\", \" \").title().strip())\n",
                "    return filtered_words\n",
                "\n",
                "def get_highest_similarity_score(wordlist,wrd):\n",
                "    score=[]\n",
                "    for each in wordlist:\n",
                "        score.append(normalized_levenshtein.similarity(each.lower(),wrd.lower()))\n",
                "    return max(score)\n",
                "\n",
                "def sense2vec_get_words(word,s2v,topn,question):\n",
                "    output = []\n",
                "    # print (\"word \",word)\n",
                "    try:\n",
                "        sense = s2v.get_best_sense(word, senses= [\"NOUN\", \"PERSON\",\"PRODUCT\",\"LOC\",\"ORG\",\"EVENT\",\"NORP\",\"WORK OF ART\",\"FAC\",\"GPE\",\"NUM\",\"FACILITY\"])\n",
                "        most_similar = s2v.most_similar(sense, n=topn)\n",
                "        output = filter_same_sense_words(sense,most_similar)\n",
                "        # print (\"Similar \",output)\n",
                "    except:\n",
                "        output =[]\n",
                "\n",
                "    threshold = 0.6\n",
                "    final=[word]\n",
                "    checklist =question.split()\n",
                "    for x in output:\n",
                "        if get_highest_similarity_score(final,x)<threshold and x not in final and x not in checklist:\n",
                "            final.append(x)\n",
                "\n",
                "    return final[1:]\n",
                "\n",
                "# def mmr(doc_embedding, word_embeddings, words, top_n, lambda_param):\n",
                "#     word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
                "#     word_similarity = cosine_similarity(word_embeddings)\n",
                "#     keywords_idx = [np.argmax(word_doc_similarity)]\n",
                "#     candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
                "#     for _ in range(top_n - 1):\n",
                "#         candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
                "#         target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
                "#         mmr = (lambda_param) * candidate_similarities - (1-lambda_param) * target_similarities.reshape(-1, 1)\n",
                "#         mmr_idx = candidates_idx[np.argmax(mmr)]\n",
                "#         keywords_idx.append(mmr_idx)\n",
                "#         candidates_idx.remove(mmr_idx)\n",
                "#     return [words[idx] for idx in keywords_idx]\n",
                "\n",
                "def get_distractors(word, origsentence, sense2vecmodel, sentencemodel, top_n, lambdaval):\n",
                "    distractors = sense2vec_get_words(word,sense2vecmodel,top_n,origsentence)\n",
                "    # print (\"distractors \",distractors)\n",
                "    # if len(distractors) ==0:\n",
                "    #     return distractors\n",
                "    # distractors_new = [word.capitalize()]\n",
                "    # distractors_new.extend(distractors)\n",
                "    # embedding_sentence = origsentence+ \" \"+word.capitalize()\n",
                "    # keyword_embedding = sentencemodel.encode([embedding_sentence])\n",
                "    # distractor_embeddings = sentencemodel.encode(distractors_new)\n",
                "    # max_keywords = min(len(distractors_new),5)\n",
                "    # filtered_keywords = mmr(keyword_embedding, distractor_embeddings,distractors_new,max_keywords,lambdaval)\n",
                "    # # filtered_keywords = filtered_keywords[1:]\n",
                "    # final = [word.capitalize()]\n",
                "    # for wrd in filtered_keywords:\n",
                "    #     if wrd.lower() != word.lower():\n",
                "    #         final.append(wrd.capitalize())\n",
                "    # final = final[1:]\n",
                "    # top1, top2, top3 = \"\", \"\", \"\"\n",
                "    # sim1, sim2, sim3 = 0, 0, 0\n",
                "    ans = []\n",
                "    for i in distractors:\n",
                "        ans.append([i, cosine_similarity(sentencemodel.encode([word]), sentencemodel.encode([i]))])\n",
                "    ans.sort(key=lambda x: x[1], reverse=True)\n",
                "    for i in range(len(ans)):\n",
                "        ans[i] = ans[i][0]\n",
                "    return ans[:3]"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "pandas",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.9"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
